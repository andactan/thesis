global:
  sequence_len: &sequence_len 16
  benchmark: &benchmark ml10

algorithm:
  class: VMPO
  args:
    epochs: 2
    minibatches: 1
    pop_art_reward_normalization: True

sampler:
  class: CpuSampler
  args:
    batch_T: *sequence_len
    batch_B: 1
    eval_n_envs: 1
    eval_max_steps: 100
    eval_max_trajectories: 100
    
    # sampling environments
    env_kwargs: &env_kwargs
      benchmark: *benchmark
      action_repeat: 2
      demonstration_action_repeat: 5
      max_trials_per_episode: 3
      mode: "meta-training"

    # evaluation environments
    eval_env_kwargs:
      <<: *env_kwargs 
      mode: "all"

runner:
  class: MinibatchRlEval
  affinity:
    n_cpu_core: 0.1
    n_gpu: 0
    set_affinity: False 
  args:
    n_steps: 4.0e+8
    log_interval_steps: 5.0e+6
  
agent:
  class: VMPOAgent
  args:
    model: 
      class: VMPOModel
      args:
        sequence_len: *sequence_len
        size: medium
        linear_value_output: False
